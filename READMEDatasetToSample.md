Starting from the dataset, once we decide on a subset of the original dataset to use and a reasonable split between training / validation / test sentences, we will organize all of the sentences we need by sentiment (political or non-political) and type (bot or human). For training and validation files, using a python script, sentences from the dataset will be added alongside their human and bot label. Specific desicions will be made as to whether unique users or the same users will be added for training and testing. For testing/evaluation/analysis tsv files, a textid number, the tweet, its target label (bot or human), and its type (political and non-political) will be recorded. With this organization, the tsv files will be prepared for utilization with NLPScholar for training, evaluation, and anlysis.
